VEEN Design Philosophy v0.0.1 (Ephemeral Fabric Tightened)
Plain ASCII, English only
	0.	Purpose and scope

This document pinpoints the design philosophy of VEEN v0.0.1 as an operating model for an “ephemeral, verifiable, reproducible network fabric”.

Motto:

Treat networks as ephemeral as pods.
A network should be disposable, verifiable, and reproducible.
A network you can clone, fork, replay, and interpret.

VEEN interprets this as:
	•	networks are infrastructure with pod-like lifetimes;
	•	hubs are disposable execution containers;
	•	logs and receipts are the durable truth;
	•	overlays (identity, wallet, RPC, CRDT, operations) are deterministic folds over logs;
	•	deployment is OS-agnostic: Linux, WSL2, Docker, k8s, k3s.

The goal is to keep these principles stable while implementations, overlays, and tooling evolve.
	1.	Primary objectives

1.1 Ephemeral fabric
	•	A “network” is treated like a pod:
	•	it can be created, scaled, drained, and destroyed without configuration debt;
	•	lifetimes are short and repeatable;
	•	the cheapest response to damage is replacement, not manual surgery.
	•	The long-lived artefacts are logs, keys, and anchor state, not hub processes, static configs, or mutable databases.

1.2 Disposability
	•	Hubs are explicitly disposable:
	•	a hub can be killed, upgraded, or moved without changing semantics of any label;
	•	a hub can be replaced by another binary using the same data directory;
	•	multiple hubs can serve the same fabric as long as they respect the core invariants.
	•	Network topologies, pod layouts, and node assignments are allowed to change frequently; logs remain authoritative.
	•	Operational guidelines should default to:
	•	“replace the hub” rather than “repair the hub”;
	•	“replay from logs” rather than “fix state in place”.

1.3 Verifiability
	•	Every accepted message is committed into an authenticated log:
	•	leaf_hash and msg_id bind ciphertext to label, profile, and client_seq;
	•	MMR roots and CHECKPOINT objects provide compact inclusion proofs;
	•	RECEIPT and CHECKPOINT carry hub signatures;
	•	anchors optionally bind MMR roots to external ledgers or trust systems.
	•	Any statement about “what happened” must be reconstructible and checkable from retained logs and proofs, without trusting a live hub, a particular database, or a specific cluster topology.

1.4 Reproducibility
	•	For any retained log prefix, folding overlays over that prefix must:
	•	produce identical state on any conforming implementation;
	•	allow state to be recomputed from scratch without hidden side channels;
	•	allow a new hub or client to join by replaying logs only.
	•	Reproducibility is preferred over mutable databases:
	•	operational state is derived, not stored ad hoc;
	•	divergence between stored state and log-derived state is treated as an error to surface, not a normal condition to tolerate.
	•	Cloneability and replayability are design requirements:
	•	copying a data directory must be sufficient to reconstruct the same fabric instance;
	•	replaying the same event sequence must yield the same overlay state.

1.5 Invariance
	•	A small set of wire objects and invariants is fixed:
	•	MSG, RECEIPT, CHECKPOINT, mmr_proof, cap_token;
	•	invariants I1..I12 over their relationships.
	•	The following must not change the meaning of these objects:
	•	OS or kernel version;
	•	container runtime (bare metal, Docker, containerd, etc.);
	•	hub process layout or topology;
	•	which overlays are in use, or how many hubs participate in the fabric.

1.6 Determinism
	•	For any finite event prefix L on a label:
	•	fold(L) is a deterministic function for every overlay;
	•	any conforming implementation produces identical overlay state from L.
	•	All non-determinism is either:
	•	forbidden, or
	•	explicitly localized and documented (for example:
	•	time in TTL checks,
	•	day-boundary limit windows,
	•	tie-breaking using stream_seq).
	•	Admission and error behavior for a given MSG under v0.0.1 are deterministic given:
	•	current log state;
	•	configured limits and profiles.
	•	Determinism is what makes “replay” and “interpret” meaningful operations rather than approximations.

1.7 Portability
	•	The same repository and binary must:
	•	run on Ubuntu 22.04 and 24.04 (native, systemd);
	•	run on WSL2 Ubuntu;
	•	run in containers under Docker, k8s, and k3s;
	•	expose identical wire semantics and log behavior.
	•	A “VEEN fabric instance” is defined by:
	•	its logs (receipts.cborseq, payloads.cborseq, checkpoints.cborseq, anchors);
	•	not by OS, container runtime, process layout, or cluster design.

1.8 Minimality
	•	Hubs:
	•	accept, verify, commit, stream, and anchor;
	•	expose health and metrics;
	•	do not interpret payload schemas.
	•	Overlays:
	•	define schemas and folding rules;
	•	are pure clients of the log model;
	•	do not require hub modifications.
	•	The core remains small enough that:
	•	audits are feasible and bounded;
	•	alternative hub implementations remain practical;
	•	cloning, forking, and replay are implementable without hidden dependencies.

1.9 Clone, fork, replay, interpret

VEEN is designed so that a network can be:
	•	Cloned:
	•	copying a fabric data directory and pinned keys is sufficient to stand up an equivalent fabric instance;
	•	cloned instances can be brought up in isolated environments for testing, audit, or simulation.
	•	Forked:
	•	retaining a log prefix and then diverging event streams allows independent evolution from a common history;
	•	forks are explicit at the fabric or label level and do not require bespoke migration paths.
	•	Replayed:
	•	overlay state is reconstructed by replaying events from zero or from a checkpoint;
	•	replay is deterministic and yields identical results for the same inputs.
	•	Interpreted:
	•	new overlays can be applied to existing logs to derive new views of the same history;
	•	no hub changes are required to interpret old logs with new overlays.

These four operations are core design targets, not side effects.
	2.	Network-as-container model

2.1 Fabric instance

A VEEN fabric instance is:
	•	a set of labels and their event logs;
	•	a set of MMR roots and checkpoints;
	•	optional anchors and bridge relationships;
	•	optional overlay-specific state reconstructed from logs.

This is analogous to:
	•	a container image plus its volume contents:
	•	logs are the durable “data volume”;
	•	hubs are disposable “container processes” that:
	•	bind logs to TCP ports and endpoints;
	•	enforce admission and invariants;
	•	interact with anchoring backends.

Cloning or snapshotting a fabric is equivalent to copying container volumes and pinned config.

2.2 Hubs as disposable nodes
	•	A hub process is started with a data directory, profile, and listen address.
	•	A hub can be:
	•	killed and restarted without semantic loss for any label whose logs are intact;
	•	replaced by a newer binary using the same data directory;
	•	replicated under supervision (systemd, k8s Deployments, k3s) with identical behavior for the same traffic and logs.
	•	Hubs are expected to be:
	•	short-lived compared to fabric lifetime;
	•	rotated often for upgrades, hardening, and topology changes.
	•	Operational guidance:
	•	“drain and replace a hub” should be routine, not exceptional.

2.3 Clients as portable endpoints
	•	A client identity (keys and state) is stored under a user-controlled path or volume.
	•	A client can:
	•	connect to any conforming hub in the fabric;
	•	resync and verify its state using /stream and CHECKPOINT endpoints;
	•	detect divergence and recover via RESYNC flows.
	•	Client correctness depends only on:
	•	retained logs;
	•	trust in pinned hub keys and profiles;
	•	never on hub-local mutable databases or in-memory caches.

	3.	Wire and invariants

3.1 Fixed wire objects
	•	MSG:
	•	carries ver, profile_id, label, client_id, client_seq, prev_ack, auth_ref, ct_hash, ciphertext, sig.
	•	RECEIPT:
	•	carries ver, label, stream_seq, leaf_hash, mmr_root, hub_ts, hub_sig.
	•	CHECKPOINT:
	•	commits label_prev, label_curr, upto_seq, mmr_root, epoch, hub_sig, optional witness_sigs.
	•	cap_token:
	•	carries issuer_pk, subject_pk, allow, sig_chain.

These objects, together with MMR structure and invariants, define what can be cloned, forked, and replayed.

3.2 Hub responsibilities

The hub must:
	•	validate:
	•	signatures (MSG.sig, hub_sig, cap_token signatures when used);
	•	sizes and limits;
	•	sequence and uniqueness invariants;
	•	profile compatibility;
	•	maintain per-label MMR state and emit RECEIPTs;
	•	emit CHECKPOINT objects and optional anchors.

The hub must not:
	•	branch on payload_hdr.schema contents for core behavior;
	•	use decrypted body for admission or ordering decisions;
	•	embed overlay-specific semantics into core wire handling.

3.3 Version stability
	•	Within v0.0.1:
	•	wire encoding is frozen;
	•	hub behavior for valid and invalid messages is frozen.
	•	Overlays may evolve by defining new schemas and folding rules without:
	•	changing MSG, RECEIPT, CHECKPOINT, mmr_proof, cap_token structure;
	•	requiring hub code changes;
	•	invalidating existing logs.

	4.	Logs as single source of truth

4.1 Append-only model
	•	The authoritative record for a label consists of:
	•	receipts.cborseq (sequence of RECEIPT, optionally paired with MSG payloads for retention);
	•	checkpoints.cborseq;
	•	optional anchors that bind mmr_root values externally.
	•	Hubs may rotate on-disk files and apply retention, but:
	•	any retained prefix must remain internally consistent and re-foldable;
	•	truncation must not silently create inconsistent prefixes;
	•	policies for retention and compaction must be documented and auditable.

4.2 Folding semantics
	•	For each overlay:
	•	state(label, upto_seq) = fold(events up to upto_seq on relevant streams).
	•	Folding rules must:
	•	depend only on:
	•	stream order (stream_seq as primary tie-breaker);
	•	payload_hdr.schema;
	•	payload body bytes;
	•	optional timestamps inside the payload body;
	•	not depend on:
	•	which hub accepted the message;
	•	sender IP or connection metadata;
	•	OS, hardware, or container runtime.
	•	Re-running fold on the same retained prefix must exactly reproduce prior overlay state, enabling replay and offline audit.

4.3 Cross-hub consistency
	•	Bridging and mirroring must preserve:
	•	payload bytes;
	•	payload_hdr.schema;
	•	a deterministic parent_id link back to original msg_id when mirroring.
	•	Overlay implementations must:
	•	treat local and bridged events under the same folding rules;
	•	document deduplication keys (for example (source_hub, label, stream_seq) or parent_id);
	•	ensure that clones and forks of fabrics that include bridged events remain foldable without ambiguity.

	5.	Overlay design constraints

5.1 Pure overlay

An overlay (identity, wallet, RPC, CRDT, operations) must:
	•	be defined only in terms of:
	•	schema identifiers in payload_hdr.schema;
	•	deterministic CBOR payload bodies;
	•	per-stream folding rules over logs;
	•	not require:
	•	hub-specific endpoints beyond VEEN core;
	•	schema-aware logic in hub;
	•	non-deterministic external state during folding.

5.2 Conflict handling

Overlays must specify:
	•	update rules:
	•	last-writer-wins when multiple updates can apply to the same key;
	•	OR-set or grow-only counter semantics when sets or counters are used;
	•	deduplication rules when duplicate events are possible (for example due to replay or mirroring);
	•	explicit tie-breakers:
	•	timestamps plus stream_seq;
	•	stream_seq alone;
	•	leaf_hash only if needed.

Any ambiguity must be resolved by documented rules. Different implementations must not diverge under the same event prefix, otherwise “interpret” ceases to be well-defined.

5.3 Compatibility
	•	Adding an overlay must not:
	•	change how MSG are accepted or ordered by the hub;
	•	require schema-specific error handling in core paths;
	•	invalidate or reinterpret older logs.
	•	Overlays must be strictly additive with respect to the core:
	•	new overlays can always be applied to old logs;
	•	old overlays continue to fold correctly when new overlays are introduced.

	6.	Identity overlay role

6.1 Identity as log-derived state
	•	Principals, devices, realms, contexts, organizations, groups, and handles:
	•	exist only as derived state from identity streams;
	•	are folded from identity messages and revocation messages.
	•	Hubs:
	•	are unaware of identity details beyond public keys and cap_token structures;
	•	are not required to understand realms, organizations, or group semantics.

6.2 Sessions and context binding
	•	Application frontends bind sessions to context identifiers using:
	•	device keys;
	•	capability chains;
	•	identity overlay logs.
	•	This binding must be:
	•	reconstructible from logs;
	•	verifiable offline;
	•	independent of hub internals.
	•	Identity overlays must remain foldable on cloned, forked, and replayed fabrics.

	7.	Wallet and paid operation overlay role

7.1 Balances and transfers from logs
	•	A wallet is defined by:
	•	wallet_id (for example realm, context, currency, account);
	•	its event stream of wallet operations and paid operations.
	•	Balances, limits, freezes, and adjustments are derived entirely by folding events:
	•	debit and credit operations;
	•	policy and configuration events;
	•	revocations and closures.

7.2 No hub-side balances
	•	Hubs do not:
	•	store balances;
	•	enforce wallet-specific rules;
	•	distinguish financial messages from other payloads.
	•	Wallet services and auditors:
	•	recompute balances from logs;
	•	detect overdrafts or violations using the same folding rules;
	•	can operate on cloned or forked fabric snapshots for what-if analysis.

	8.	OS and container integration

8.1 Execution environment
	•	VEEN must run as:
	•	a normal Unix process on Linux and WSL2;
	•	a container entrypoint in Docker, k8s, and k3s;
	•	the same veen binary with the same CLI surface.
	•	No mandatory external persistent systems:
	•	no required RDBMS;
	•	no required message broker;
	•	no required external KMS.
	•	Integrations with databases, KMS, or queues are permitted as overlays or sidecars, not as core dependencies.

8.2 Data and logs
	•	Hub data directories:
	•	are plain directories, mountable as volumes;
	•	contain receipts, payloads, checkpoints, anchor metadata, and state files;
	•	can be attached to containers or bare-metal processes interchangeably.
	•	Clients:
	•	store identity and state under user paths or mounted volumes;
	•	must be able to move between machines without changing semantics, as long as keys and state are preserved.

8.3 Supervision and lifecycle
	•	Hub lifecycle is managed by:
	•	systemd units on bare metal and WSL2;
	•	Deployments or StatefulSets on k8s and k3s;
	•	equivalent constructs on other orchestrators.
	•	Shutdown must:
	•	flush receipts and checkpoints to disk at documented durability guarantees;
	•	close listeners cleanly;
	•	permit immediate restart on the same data directory and port.
	•	Operational practices should treat hub restarts and redeployments as normal background noise, not rare events.

	9.	Security model

9.1 Trust boundaries
	•	Clients trust:
	•	their own keys and keystore;
	•	pinned hub public keys and profiles;
	•	log ordering and hub signatures as enforced by core invariants.
	•	Hubs are trusted to:
	•	enforce admission via cap_token and rate limiting;
	•	maintain MMR consistency and signatures;
	•	respect configured limits.
	•	Hubs are not trusted with:
	•	plaintext application payloads;
	•	overlay semantics or policy decisions beyond admission.

9.2 Confidentiality and integrity
	•	Confidentiality:
	•	payloads are protected by AEAD using per-label keys derived from HPKE contexts;
	•	hub does not see decrypted payload_hdr or body.
	•	Integrity:
	•	client-level via MSG.sig over the canonical MSG encoding (without sig);
	•	hub-level via hub_sig over RECEIPT and CHECKPOINT;
	•	log-level via MMR roots and inclusion proofs plus optional external anchoring.
	•	These properties must hold across clones, forks, and replays of the fabric.

9.3 Revocation and rotation
	•	Revocation is represented as overlay events and cap_token lifecycle:
	•	device or client identity revocations;
	•	capability and auth_ref revocations.
	•	Hubs and applications:
	•	read the same revocation events;
	•	enforce consistent decisions for new messages.
	•	Key rotation:
	•	is performed without changing wire semantics;
	•	is recorded via CHECKPOINT witness signatures and overlay events;
	•	must remain verifiable when logs are cloned, forked, or replayed.

	10.	Evolution and versioning

10.1 Core evolution
	•	VEEN v0.0.1:
	•	fixes wire objects, invariants, and error behavior.
	•	Future core versions may:
	•	add new fields or error codes;
	•	define new invariants;
	•	provide explicit upgrade paths.
	•	Core evolution must not:
	•	silently reinterpret v0.0.1 logs;
	•	break deterministic folding for pre-existing schemas;
	•	invalidate the ability to clone, fork, replay, and interpret v0.0.1 fabrics.

10.2 Overlay evolution
	•	New overlays are added by:
	•	defining new schema identifiers;
	•	defining folding rules and invariants for those schemas;
	•	documenting interaction with existing overlays.
	•	Existing overlays may:
	•	evolve via versioned schemas (for example “id.ctx.profile.v2”);
	•	introduce new fields while keeping old behavior well-defined.
	•	Overlay evolution must:
	•	be replayable from combined logs;
	•	specify precedence and tie-breaking between versions;
	•	preserve interpretability of past events.

	11.	Non-goals

VEEN does not aim to be:
	•	a general-purpose compute or function execution platform;
	•	a consensus or blockchain system;
	•	a deep packet inspection or routing engine;
	•	a replacement for every existing authentication or identity standard;
	•	a full-featured API gateway or service mesh.

VEEN aims to be:
	•	a minimal, deterministic fabric for:
	•	encrypted event transport;
	•	verifiable logging;
	•	overlay-defined state;
	•	a network layer that is:
	•	as ephemeral as pods in deployment;
	•	as auditable as a ledger in history;
	•	cloneable, forkable, replayable, and re-interpretable in operation.

	12.	Summary axis

The design axis that must not drift is:
	•	Fix a minimal wire and log core with strong invariants.
	•	Treat logs, receipts, and checkpoints as the single durable truth.
	•	Make hubs disposable containers that attach logs to networks and enforce invariants.
	•	Keep all higher-level semantics (identity, wallet, RPC, CRDT, operations) as deterministic folds over logs.
	•	Require OS and container independence: Linux, WSL2, Docker, k8s, and k3s all yield identical semantics for the same logs.
	•	Ensure that any conforming implementation supports:
	•	cloning fabrics by copying data;
	•	forking fabrics by splitting histories;
	•	replaying fabrics by folding logs;
	•	interpreting fabrics by adding overlays.

As long as these properties hold, networks can be treated as ephemeral as pods: disposable in deployment, verifiable in audit, reproducible under replay, and interpretable under new overlays.
